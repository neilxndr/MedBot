import tensorflow as tf

class Train:
    def __init__(self):
        self.optimizer = tf.keras.optimizers.Adam()
        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True, reduction='none')

    '''
    real: these are the actual labels 
    pred: these are the prediction, generated by the model
    '''
    def loss_function(self, real, pred):
        mask_local = tf.math.logical_not(tf.math.equal(real, 0))
        loss_fn = self.loss_object(real, pred)

        mask = tf.cast(mask_local, dtype=loss_fn.dtype)
        loss_fn *= mask
        return tf.reduce_mean(loss_fn)

    '''
    train: the training data
    label: labels of the training data
    BITCH_SIZE: the batch size of training and label data
    enc_hidden: initial input hidden stated of encoder
    encoder: encoder model
    decoder: decoder model
    label_tokenizer: token of all words present in the label dataset.
    '''
    def train_step(self, train, label, enc_hidden, encoder, decoder, BATCH_SIZE, label_tokenizer):
        loss = 0 # initial loss is equal to 0

        with tf.GradientTape() as tape: # gradient tape use to store the gradient values
            enc_output, enc_hidden = encoder(train, enc_hidden) # proceed with encoder outputs, and final hidden layer

            dec_hidden = enc_hidden # encoder hidden state provided to the attention layer present in the decoder.
            dec_input = tf.expand_dims([label_tokenizer.word_index['<start>']] * BATCH_SIZE, 1) # Initial decoder input of <start> tag index

            # calculate the loss for each word present in the sentence of the label data.
            for t in range(1, label.shape[1]):
                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)
                loss += self.loss_function(label[:, t], predictions)

                # after a loop the decoder input is equal to the index of the previous word.
                dec_input = tf.expand_dims(label[:, t], 1)

        batch_loss = (loss / int(label.shape[1]))

        variables = encoder.trainable_variables + decoder.trainable_variables
        gradients = tape.gradient(loss, variables)
        self.optimizer.apply_gradients(zip(gradients, variables))

        return batch_loss

if __name__ == '__main__':
    print('oot sssd train')